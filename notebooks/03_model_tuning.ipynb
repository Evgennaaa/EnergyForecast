{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 3: Тюнинг моделей\n",
    "\n",
    "Этот ноутбук содержит:\n",
    "- Тюнинг Random Forest (RandomizedSearchCV)\n",
    "- Тюнинг XGBoost (RandomizedSearchCV)\n",
    "- Тюнинг LightGBM (RandomizedSearchCV)\n",
    "- Сравнение результатов тюнинга\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных и функций\n",
    "\n",
    "Этот ноутбук требует:\n",
    "1. Подготовленные данные из `01_data_analysis.ipynb`\n",
    "2. Функции генерации признаков из `02_model_comparison.ipynb`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4Ify9A6hq4l"
   },
   "source": [
    "для тюнинга оставляем Random Forest, XGBoost, LightGBM??, LSTM???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ML\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " View 'ml_features' загружена из кеша: X.shape=(34896, 34)\n",
      "Данные готовы для тюнинга: X.shape=(34896, 34)\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(str(Path().resolve().parent))\n",
    "\n",
    "from src.data.data_manager import DataManager\n",
    "from src.data.feature_builder import build_targets_multi_horizon, time_split_Xy\n",
    "\n",
    "\n",
    "try:\n",
    "    dm  # Если уже создан в предыдущем ноутбуке\n",
    "    X, y  # Если уже загружены\n",
    "    targets  # Если уже созданы\n",
    "except NameError:\n",
    "    dm = DataManager()\n",
    "    X, y = dm.get_ml_features_view()\n",
    "    targets = build_targets_multi_horizon(y, horizons=(1, 24, 168))\n",
    "\n",
    "print(f\"Данные готовы для тюнинга: X.shape={X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_splits_multi_horizon(\n",
    "    X: pd.DataFrame,\n",
    "    targets: dict,\n",
    "    horizons=(1, 24, 168),\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15\n",
    "):\n",
    "\n",
    "    splits = {}\n",
    "\n",
    "    for h in horizons:\n",
    "        mask = targets[h][\"mask\"]\n",
    "        X_h = X.loc[mask]\n",
    "        y_h = targets[h][\"y\"].loc[mask]\n",
    "\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = time_split_Xy(\n",
    "            X_h, y_h, train_ratio=train_ratio, val_ratio=val_ratio\n",
    "        )\n",
    "\n",
    "        splits[h] = (X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "splits = build_splits_multi_horizon(X, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/metadata/splits.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(\"models/metadata\").mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(splits, \"models/metadata/splits.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(y_true, y_pred, eps=1e-8):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    \n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred) + eps)\n",
    "    \n",
    "    return float(100.0 * np.mean(2.0 * numerator / denominator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf3a0d53d25428090d5cb23765fae98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RF tuning by horizon:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[RF tuned] h=1: val_MAE=108.78, val_RMSE=142.33, val_sMAPE=11.92% | tune=969.9s, infer=0.127s\n",
      "  best_params: {'n_estimators': 600, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 30, 'bootstrap': True}\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[RF tuned] h=24: val_MAE=112.23, val_RMSE=146.13, val_sMAPE=12.25% | tune=957.9s, infer=0.080s\n",
      "  best_params: {'n_estimators': 400, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': None, 'bootstrap': True}\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[RF tuned] h=168: val_MAE=112.24, val_RMSE=146.37, val_sMAPE=12.30% | tune=929.3s, infer=0.049s\n",
      "  best_params: {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 20, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "def tune_rf_multi_horizon_v3(\n",
    "    splits: dict,                 \n",
    "    horizons=(1, 24, 168),\n",
    "    n_splits=5,\n",
    "    n_iter=30,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    random_state=42,\n",
    "    rf_n_jobs=-1,\n",
    "    search_n_jobs=1,\n",
    "    verbose=1,\n",
    "    store_best_estimator=False\n",
    "):\n",
    "    results = {}\n",
    "\n",
    "    param_dist = {\n",
    "        \"n_estimators\": [200, 400, 600],\n",
    "        \"max_depth\": [None, 10, 20, 30],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"max_features\": [\"sqrt\", 0.5, 0.8],\n",
    "        \"bootstrap\": [True],\n",
    "    }\n",
    "\n",
    "    for h in tqdm(horizons, desc=\"RF tuning by horizon\"):\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = splits[h]\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "        rf = RandomForestRegressor(\n",
    "            random_state=random_state,\n",
    "            n_jobs=rf_n_jobs\n",
    "        )\n",
    "\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=rf,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=n_iter,\n",
    "            scoring=scoring,\n",
    "            cv=tscv,\n",
    "            random_state=random_state,\n",
    "            n_jobs=search_n_jobs,\n",
    "            verbose=verbose,\n",
    "            refit=True,\n",
    "            return_train_score=False\n",
    "        )\n",
    "\n",
    "        t0 = time.time()\n",
    "        search.fit(X_train, y_train)\n",
    "        tuning_time_sec = time.time() - t0\n",
    "\n",
    "        best_model = search.best_estimator_\n",
    "\n",
    "        t0 = time.time()\n",
    "        y_pred_val = best_model.predict(X_val)\n",
    "        inference_time_sec = time.time() - t0\n",
    "\n",
    "        mae = mean_absolute_error(y_val, y_pred_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
    "        smape_pct = smape(y_val.values, y_pred_val)\n",
    "\n",
    "        results[h] = {\n",
    "            \"best_params\": search.best_params_,\n",
    "            \"cv_best_score\": search.best_score_,\n",
    "            \"val_MAE\": mae,\n",
    "            \"val_RMSE\": rmse,\n",
    "            \"val_sMAPE_pct\": smape_pct,\n",
    "            \"tuning_time_sec\": tuning_time_sec,\n",
    "            \"inference_time_sec\": inference_time_sec,\n",
    "            \"n_train\": len(X_train),\n",
    "            \"n_val\": len(X_val),\n",
    "            \"n_test\": len(X_test),\n",
    "        }\n",
    "\n",
    "        if store_best_estimator:\n",
    "            results[h][\"best_estimator\"] = best_model\n",
    "\n",
    "        print(\n",
    "            f\"[RF tuned] h={h}: \"\n",
    "            f\"val_MAE={mae:.2f}, val_RMSE={rmse:.2f}, val_sMAPE={smape_pct:.2f}% | \"\n",
    "            f\"tune={tuning_time_sec:.1f}s, infer={inference_time_sec:.3f}s\"\n",
    "        )\n",
    "        print(f\"  best_params: {search.best_params_}\")\n",
    "\n",
    "        del search, best_model\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "rf_tuned = tune_rf_multi_horizon_v3(splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8821e3bd302d43f3bd81371ccb6c3ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "XGB tuning by horizon:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[XGB tuned] h=1: val_MAE=108.86, val_RMSE=142.69, val_sMAPE=11.93% | tune=362.3s, infer=0.035s\n",
      "  best_params: {'subsample': 0.6, 'reg_lambda': 1.0, 'reg_alpha': 0.001, 'n_estimators': 500, 'min_child_weight': 5, 'max_depth': 6, 'learning_rate': 0.01, 'gamma': 1.0, 'colsample_bytree': 0.8}\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[XGB tuned] h=24: val_MAE=114.73, val_RMSE=149.05, val_sMAPE=12.54% | tune=418.4s, infer=0.019s\n",
      "  best_params: {'subsample': 0.6, 'reg_lambda': 2.0, 'reg_alpha': 0.1, 'n_estimators': 500, 'min_child_weight': 7, 'max_depth': 3, 'learning_rate': 0.01, 'gamma': 0.1, 'colsample_bytree': 0.6}\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[XGB tuned] h=168: val_MAE=110.92, val_RMSE=145.05, val_sMAPE=12.11% | tune=430.3s, infer=0.035s\n",
      "  best_params: {'subsample': 0.6, 'reg_lambda': 1.0, 'reg_alpha': 0.001, 'n_estimators': 500, 'min_child_weight': 5, 'max_depth': 6, 'learning_rate': 0.01, 'gamma': 1.0, 'colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "def tune_xgb_multi_horizon_v3(\n",
    "    splits: dict,                   \n",
    "    horizons=(1, 24, 168),\n",
    "    n_splits=5,\n",
    "    n_iter=30,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    random_state=42,\n",
    "    xgb_n_jobs=1,                   \n",
    "    search_n_jobs=1,                \n",
    "    verbose=1,\n",
    "    store_best_estimator=False\n",
    "):\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    param_dist = {\n",
    "        \"n_estimators\": [300, 500, 800, 1200],\n",
    "        \"learning_rate\": [0.01, 0.03, 0.05, 0.1],\n",
    "        \"max_depth\": [3, 4, 5, 6, 8],\n",
    "        \"min_child_weight\": [1, 3, 5, 7],\n",
    "        \"subsample\": [0.6, 0.8, 1.0],\n",
    "        \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "        \"gamma\": [0, 0.1, 0.3, 1.0],\n",
    "        \"reg_alpha\": [0, 1e-3, 1e-2, 0.1],\n",
    "        \"reg_lambda\": [1.0, 2.0, 5.0, 10.0],\n",
    "    }\n",
    "\n",
    "    for h in tqdm(horizons, desc=\"XGB tuning by horizon\"):\n",
    "        if h not in splits:\n",
    "            raise KeyError(f\"splits не содержит горизонт {h}. Доступные: {list(splits.keys())}\")\n",
    "\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = splits[h]\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "        xgb = XGBRegressor(\n",
    "            objective=\"reg:squarederror\",\n",
    "            tree_method=\"hist\",      \n",
    "            random_state=random_state,\n",
    "            n_jobs=xgb_n_jobs,\n",
    "            verbosity=0\n",
    "        )\n",
    "\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=xgb,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=n_iter,\n",
    "            scoring=scoring,\n",
    "            cv=tscv,\n",
    "            random_state=random_state,\n",
    "            n_jobs=search_n_jobs,\n",
    "            verbose=verbose,\n",
    "            refit=True,\n",
    "            return_train_score=False\n",
    "        )\n",
    "\n",
    "        t0 = time.time()\n",
    "        search.fit(X_train, y_train)\n",
    "        tuning_time_sec = time.time() - t0\n",
    "\n",
    "        best_model = search.best_estimator_\n",
    "\n",
    "        t0 = time.time()\n",
    "        y_pred_val = best_model.predict(X_val)\n",
    "        inference_time_sec = time.time() - t0\n",
    "\n",
    "        mae = mean_absolute_error(y_val, y_pred_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
    "        smape_pct = smape(y_val.values, y_pred_val)\n",
    "\n",
    "        results[h] = {\n",
    "            \"best_params\": search.best_params_,\n",
    "            \"cv_best_score\": search.best_score_,\n",
    "            \"val_MAE\": mae,\n",
    "            \"val_RMSE\": rmse,\n",
    "            \"val_sMAPE_pct\": smape_pct,\n",
    "            \"tuning_time_sec\": tuning_time_sec,\n",
    "            \"inference_time_sec\": inference_time_sec,\n",
    "            \"n_train\": len(X_train),\n",
    "            \"n_val\": len(X_val),\n",
    "            \"n_test\": len(X_test),\n",
    "        }\n",
    "\n",
    "        if store_best_estimator:\n",
    "            results[h][\"best_estimator\"] = best_model\n",
    "\n",
    "        print(\n",
    "            f\"[XGB tuned] h={h}: \"\n",
    "            f\"val_MAE={mae:.2f}, val_RMSE={rmse:.2f}, val_sMAPE={smape_pct:.2f}% | \"\n",
    "            f\"tune={tuning_time_sec:.1f}s, infer={inference_time_sec:.3f}s\"\n",
    "        )\n",
    "        print(f\"  best_params: {search.best_params_}\")\n",
    "\n",
    "        del best_model, search\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "xgb_tuned = tune_xgb_multi_horizon_v3(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a0cd1c2a814f2da6938ad0810b4f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LGBM tuning by horizon:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[LGBM tuned] h=1: MAE=108.28, RMSE=142.44, sMAPE=11.86% | tune_time=443.2s, val_pred_time=0.261s\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[LGBM tuned] h=24: MAE=111.96, RMSE=145.89, sMAPE=12.24% | tune_time=440.8s, val_pred_time=0.275s\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[LGBM tuned] h=168: MAE=110.87, RMSE=145.35, sMAPE=12.11% | tune_time=438.9s, val_pred_time=0.154s\n"
     ]
    }
   ],
   "source": [
    "def tune_lgbm_multi_horizon_v3(\n",
    "    splits: dict,                 \n",
    "    horizons=(1, 24, 168),\n",
    "    n_splits=5,\n",
    "    n_iter=30,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    random_state=42,\n",
    "    n_jobs_cv=1,                 \n",
    "    verbose=1\n",
    "):\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for h in tqdm(horizons, desc=\"LGBM tuning by horizon\"):\n",
    "        if h not in splits:\n",
    "            raise KeyError(f\"splits не содержит горизонт {h}. Доступные: {list(splits.keys())}\")\n",
    "\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = splits[h]\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "        base_model = LGBMRegressor(\n",
    "            objective=\"regression\",\n",
    "            random_state=random_state,\n",
    "            n_jobs=1,\n",
    "            verbose=-1\n",
    "        )\n",
    "\n",
    "        param_dist = {\n",
    "            \"n_estimators\": [400, 800, 1200, 2000],\n",
    "            \"learning_rate\": [0.01, 0.03, 0.05, 0.1],\n",
    "            \"num_leaves\": [31, 63, 127, 255],\n",
    "            \"max_depth\": [-1, 6, 8, 10, 12],\n",
    "            \"min_child_samples\": [10, 20, 40, 80],\n",
    "            \"subsample\": [0.6, 0.8, 1.0],\n",
    "            \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "            \"reg_alpha\": [0.0, 1e-3, 1e-2, 0.1],\n",
    "            \"reg_lambda\": [0.0, 0.1, 1.0, 5.0, 10.0],\n",
    "            \"bagging_freq\": [0, 1],\n",
    "        }\n",
    "\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=base_model,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=n_iter,\n",
    "            scoring=scoring,\n",
    "            cv=tscv,\n",
    "            random_state=random_state,\n",
    "            n_jobs=n_jobs_cv,\n",
    "            verbose=verbose,\n",
    "            refit=True\n",
    "        )\n",
    "\n",
    "        start_tune = time.time()\n",
    "        search.fit(X_train, y_train)\n",
    "        tuning_time_sec = time.time() - start_tune\n",
    "\n",
    "        best_model = search.best_estimator_\n",
    "\n",
    "        start_pred = time.time()\n",
    "        y_pred_val = best_model.predict(X_val)\n",
    "        inference_time_sec = time.time() - start_pred\n",
    "\n",
    "        mae = mean_absolute_error(y_val, y_pred_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
    "        smape_pct = smape(y_val.values, y_pred_val)\n",
    "\n",
    "        results[h] = {\n",
    "            \"best_params\": search.best_params_,\n",
    "            \"cv_best_score\": search.best_score_,\n",
    "            \"val_MAE\": mae,\n",
    "            \"val_RMSE\": rmse,\n",
    "            \"val_sMAPE_pct\": smape_pct,\n",
    "            \"tuning_time_sec\": tuning_time_sec,\n",
    "            \"inference_time_sec\": inference_time_sec,\n",
    "            \"n_train\": len(X_train),\n",
    "            \"n_val\": len(X_val),\n",
    "            \"n_test\": len(X_test),\n",
    "        }\n",
    "\n",
    "        print(\n",
    "            f\"[LGBM tuned] h={h}: \"\n",
    "            f\"MAE={mae:.2f}, RMSE={rmse:.2f}, sMAPE={smape_pct:.2f}% | \"\n",
    "            f\"tune_time={tuning_time_sec:.1f}s, \"\n",
    "            f\"val_pred_time={inference_time_sec:.3f}s\"\n",
    "        )\n",
    "\n",
    "        del best_model, search\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "lgbm_tuned = tune_lgbm_multi_horizon_v3(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_tuning_results(\n",
    "    rf_results: dict,\n",
    "    xgb_results: dict,\n",
    "    lgbm_results: dict\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for model_name, results in [\n",
    "        (\"RandomForest\", rf_results),\n",
    "        (\"XGBoost\", xgb_results),\n",
    "        (\"LightGBM\", lgbm_results),\n",
    "    ]:\n",
    "        for h, res in results.items():\n",
    "            rows.append({\n",
    "                \"model\": model_name,\n",
    "                \"horizon\": h,\n",
    "                \"val_MAE\": res.get(\"val_MAE\"),\n",
    "                \"val_RMSE\": res.get(\"val_RMSE\"),\n",
    "                \"val_sMAPE_pct\": res.get(\"val_sMAPE_pct\"),\n",
    "                \"tuning_time_sec\": res.get(\"tuning_time_sec\"),\n",
    "                \"inference_time_sec\": res.get(\"inference_time_sec\"),\n",
    "                \"n_train\": res.get(\"n_train\"),\n",
    "                \"n_val\": res.get(\"n_val\"),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    df = df.sort_values([\"horizon\", \"model\"]).reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>horizon</th>\n",
       "      <th>val_MAE</th>\n",
       "      <th>val_RMSE</th>\n",
       "      <th>val_sMAPE_pct</th>\n",
       "      <th>tuning_time_sec</th>\n",
       "      <th>inference_time_sec</th>\n",
       "      <th>n_train</th>\n",
       "      <th>n_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>1</td>\n",
       "      <td>108.276612</td>\n",
       "      <td>142.437545</td>\n",
       "      <td>11.863293</td>\n",
       "      <td>443.196134</td>\n",
       "      <td>0.261393</td>\n",
       "      <td>24426</td>\n",
       "      <td>5234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>1</td>\n",
       "      <td>108.781393</td>\n",
       "      <td>142.332314</td>\n",
       "      <td>11.918301</td>\n",
       "      <td>969.870981</td>\n",
       "      <td>0.127348</td>\n",
       "      <td>24426</td>\n",
       "      <td>5234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>1</td>\n",
       "      <td>108.858475</td>\n",
       "      <td>142.694043</td>\n",
       "      <td>11.931903</td>\n",
       "      <td>362.284240</td>\n",
       "      <td>0.035030</td>\n",
       "      <td>24426</td>\n",
       "      <td>5234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>24</td>\n",
       "      <td>111.964110</td>\n",
       "      <td>145.894578</td>\n",
       "      <td>12.237998</td>\n",
       "      <td>440.831654</td>\n",
       "      <td>0.275194</td>\n",
       "      <td>24410</td>\n",
       "      <td>5230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>24</td>\n",
       "      <td>112.228788</td>\n",
       "      <td>146.132868</td>\n",
       "      <td>12.252270</td>\n",
       "      <td>957.890449</td>\n",
       "      <td>0.079984</td>\n",
       "      <td>24410</td>\n",
       "      <td>5230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>24</td>\n",
       "      <td>114.729294</td>\n",
       "      <td>149.050347</td>\n",
       "      <td>12.535573</td>\n",
       "      <td>418.449609</td>\n",
       "      <td>0.018852</td>\n",
       "      <td>24410</td>\n",
       "      <td>5230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>168</td>\n",
       "      <td>110.874462</td>\n",
       "      <td>145.353144</td>\n",
       "      <td>12.113894</td>\n",
       "      <td>438.853887</td>\n",
       "      <td>0.153938</td>\n",
       "      <td>24309</td>\n",
       "      <td>5209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>168</td>\n",
       "      <td>112.242838</td>\n",
       "      <td>146.374374</td>\n",
       "      <td>12.304519</td>\n",
       "      <td>929.290376</td>\n",
       "      <td>0.048533</td>\n",
       "      <td>24309</td>\n",
       "      <td>5209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>168</td>\n",
       "      <td>110.919487</td>\n",
       "      <td>145.054179</td>\n",
       "      <td>12.112122</td>\n",
       "      <td>430.301112</td>\n",
       "      <td>0.034753</td>\n",
       "      <td>24309</td>\n",
       "      <td>5209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model  horizon     val_MAE    val_RMSE  val_sMAPE_pct  \\\n",
       "0      LightGBM        1  108.276612  142.437545      11.863293   \n",
       "1  RandomForest        1  108.781393  142.332314      11.918301   \n",
       "2       XGBoost        1  108.858475  142.694043      11.931903   \n",
       "3      LightGBM       24  111.964110  145.894578      12.237998   \n",
       "4  RandomForest       24  112.228788  146.132868      12.252270   \n",
       "5       XGBoost       24  114.729294  149.050347      12.535573   \n",
       "6      LightGBM      168  110.874462  145.353144      12.113894   \n",
       "7  RandomForest      168  112.242838  146.374374      12.304519   \n",
       "8       XGBoost      168  110.919487  145.054179      12.112122   \n",
       "\n",
       "   tuning_time_sec  inference_time_sec  n_train  n_val  \n",
       "0       443.196134            0.261393    24426   5234  \n",
       "1       969.870981            0.127348    24426   5234  \n",
       "2       362.284240            0.035030    24426   5234  \n",
       "3       440.831654            0.275194    24410   5230  \n",
       "4       957.890449            0.079984    24410   5230  \n",
       "5       418.449609            0.018852    24410   5230  \n",
       "6       438.853887            0.153938    24309   5209  \n",
       "7       929.290376            0.048533    24309   5209  \n",
       "8       430.301112            0.034753    24309   5209  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df = collect_tuning_results(\n",
    "    rf_results=rf_tuned,\n",
    "    xgb_results=xgb_tuned,\n",
    "    lgbm_results=lgbm_tuned\n",
    ")\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/metadata/xgb_tuned_results.joblib']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(\"models/metadata\").mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(xgb_tuned, \"models/metadata/xgb_tuned_results.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artifacts/metadata/lgbm_tuned_results.joblib']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(\"artifacts/metadata\").mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(lgbm_tuned, \"artifacts/metadata/lgbm_tuned_results.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (EnergyForecast)",
   "language": "python",
   "name": "energyforecast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
